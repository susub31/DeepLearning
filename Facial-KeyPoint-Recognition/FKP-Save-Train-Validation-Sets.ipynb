{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Key Point Detection\n",
    "\n",
    "This is using the Kaggle Dataset, wherein the training data has labels for 15 different coordinates (x,y) on an individual's face, making it 30 labels in total.  This is so because, these are the key points that help identify an individual's face.\n",
    "\n",
    "The goal of this exercise is to build a Deep Neural Network using Keras and make predictions on the validation / test datasets.  Validation dataset is part (20%) of the original training dataset that was identified as good ones with all labels.  Test dataset is the original test set from Kaggle website and does not have labels for us to validate.  However, we can predict on the test set and plot the predictions on the images to visually validate our model's performance.\n",
    "\n",
    "We will use Keras library for this exercise and try tuning several hyper-parameters to identify what gets best results.  Hyper-parameters tuned includes optimizers, filters, kernel size, number of convolutional layers etc.  Further enhancements can be done by applying blurring, contrast enhancements etc. using image processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The following function loads the dataset and does clean-up to exclude ones with missing labels.  The training data and labels are then returned to the calling function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(test=False):\n",
    "    \"\"\" Function to load the dataset into np arrays\n",
    "    \n",
    "        Argument: \n",
    "        test - boolean value to indicate 'test' if True and 'training' if False \n",
    "\n",
    "        Returns: \n",
    "        X: np array holding training / test data\n",
    "        y: np array holding labels\n",
    "        cols: column names (30 data points that are labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # files for training and test datasets\n",
    "    FTRAIN = 'training/training.csv'\n",
    "    FTEST = 'test/test.csv'\n",
    "    \n",
    "    filename = FTEST if test else FTRAIN\n",
    "\n",
    "    df = read_csv(os.path.expanduser(filename))\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "    cols = df.columns\n",
    "    \n",
    "    # normalize values\n",
    "    X = np.vstack(df['Image'].values)/255.\n",
    "\n",
    "    # labels missing in training should be removed from training\n",
    "    if (test==False):\n",
    "        y = df[df.columns[0:30]].values\n",
    "        X = X[~ np.isnan(y).any(axis=1)]\n",
    "        y = y[~ np.isnan(y).any(axis=1)]\n",
    "        X, y = shuffle(X, y, random_state=42)\n",
    "    else:\n",
    "        y = None\n",
    "        cols = None\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    \n",
    "    # return X (data), y (labels) and cols (column names)\n",
    "    return X, y, cols\n",
    "\n",
    "def load_2D(test=False):\n",
    "    \"\"\" Load into 2D by reshaping\n",
    "    \n",
    "        Argument: \n",
    "        test - boolean value to indicate 'test' if True and 'training' if False \n",
    "\n",
    "        Returns: \n",
    "        X: np array holding training / test data\n",
    "        y: np array holding labels\n",
    "        cols: column names (30 data points that are labels)\n",
    "    \"\"\"\n",
    "    X, y, cols = load(test)\n",
    "    X = X.reshape(-1, 96, 96, 1)\n",
    "    \n",
    "    return X, y, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "X_train, y_train, label_cols = load_2D(test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implicitly split the training dataset into 'train' and 'validation' datasets (test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 96, 96, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train.reshape(1712,9216))\n",
    "df_train.to_csv('new_training.csv', index=False)\n",
    "#fkp_id[['RowId', 'Location']].to_csv('Submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.DataFrame(X_valid.reshape(428,9216))\n",
    "df_valid.to_csv('new_validation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels = pd.DataFrame(y_train)\n",
    "df_train_labels.to_csv('new_training_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_labels = pd.DataFrame(y_valid)\n",
    "df_valid_labels.to_csv('new_validation_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62.76970213, 35.51509787, 30.51574468, ..., 68.07285106,\n",
       "        48.16408511, 79.02706383],\n",
       "       [65.39765227, 36.96986932, 31.02671096, ..., 72.11070393,\n",
       "        51.36959342, 77.76892257],\n",
       "       [64.76246617, 34.93338947, 31.85972932, ..., 69.44914286,\n",
       "        52.18213534, 83.32006015],\n",
       "       ...,\n",
       "       [66.51187302, 30.94238549, 29.68014512, ..., 75.56190476,\n",
       "        62.43809524, 82.5170068 ],\n",
       "       [67.64129032, 37.1708129 , 31.65754839, ..., 72.11605161,\n",
       "        49.99587097, 84.22606452],\n",
       "       [64.69317293, 35.07406917, 28.56396992, ..., 63.46105263,\n",
       "        48.88637594, 75.39681203]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('new_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1712, 9216)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1712, 96, 96, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new1 = np.array(df_new)\n",
    "df_new1 = df_new.reshape(1712, 96,96, 1)\n",
    "df_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (1712, 96, 96, 1)  and Train label shape:  (1712, 30)\n",
      "Labels:  Index(['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x',\n",
      "       'right_eye_center_y', 'left_eye_inner_corner_x',\n",
      "       'left_eye_inner_corner_y', 'left_eye_outer_corner_x',\n",
      "       'left_eye_outer_corner_y', 'right_eye_inner_corner_x',\n",
      "       'right_eye_inner_corner_y', 'right_eye_outer_corner_x',\n",
      "       'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x',\n",
      "       'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x',\n",
      "       'left_eyebrow_outer_end_y', 'right_eyebrow_inner_end_x',\n",
      "       'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x',\n",
      "       'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y',\n",
      "       'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x',\n",
      "       'mouth_right_corner_y', 'mouth_center_top_lip_x',\n",
      "       'mouth_center_top_lip_y', 'mouth_center_bottom_lip_x',\n",
      "       'mouth_center_bottom_lip_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data shape: \", X_train.shape, \" and Train label shape: \", y_train.shape)\n",
    "print(\"Labels: \", label_cols[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset shape:  (428, 96, 96, 1)  and Validation label shape:  (428, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset shape: \", X_valid.shape, \" and Validation label shape: \", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "X_test, y_test, label_cols = load_2D(test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (1783, 96, 96, 1)  and Test label shape:  None\n",
      "Labels:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data shape: \", X_test.shape, \" and Test label shape: \", y_test)\n",
    "print(\"Labels: \", label_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
